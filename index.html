<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Web Whisper - Real-time Transcription</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background-color: #1a1a1a;
            color: #e0e0e0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }

        .header {
            background-color: #2a2a2a;
            padding: 1rem 2rem;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
        }

        .header h1 {
            font-size: 1.5rem;
            font-weight: 600;
            color: #ffffff;
        }

        .container {
            flex: 1;
            padding: 2rem;
            max-width: 1200px;
            width: 100%;
            margin: 0 auto;
        }

        .status-bar {
            background-color: #2a2a2a;
            border-radius: 8px;
            padding: 1rem 1.5rem;
            margin-bottom: 2rem;
            display: flex;
            align-items: center;
            justify-content: space-between;
            flex-wrap: wrap;
            gap: 1rem;
        }

        .status-indicator {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .status-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
            background-color: #666;
            transition: background-color 0.3s ease;
        }

        .status-dot.active {
            background-color: #4CAF50;
            animation: pulse 2s infinite;
        }

        .status-dot.error {
            background-color: #f44336;
        }

        @keyframes pulse {
            0% {
                box-shadow: 0 0 0 0 rgba(76, 175, 80, 0.7);
            }
            70% {
                box-shadow: 0 0 0 10px rgba(76, 175, 80, 0);
            }
            100% {
                box-shadow: 0 0 0 0 rgba(76, 175, 80, 0);
            }
        }

        .audio-level {
            width: 100px;
            height: 6px;
            background-color: #333;
            border-radius: 3px;
            overflow: hidden;
            margin-left: 1rem;
        }

        .audio-level-bar {
            height: 100%;
            background-color: #4CAF50;
            width: 0%;
            transition: width 0.1s ease;
        }

        .api-key-section {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .api-key-input {
            background-color: #1a1a1a;
            border: 1px solid #444;
            color: #e0e0e0;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            font-size: 0.9rem;
            width: 300px;
        }

        .api-key-input:focus {
            outline: none;
            border-color: #4CAF50;
        }

        .save-key-btn {
            background-color: #4CAF50;
            color: white;
            border: none;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s ease;
        }

        .save-key-btn:hover {
            background-color: #45a049;
        }

        .transcription-area {
            background-color: #2a2a2a;
            border-radius: 8px;
            padding: 1.5rem;
            height: calc(100vh - 300px);
            min-height: 400px;
            display: flex;
            flex-direction: column;
        }

        .transcription-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
        }

        .transcription-header h2 {
            font-size: 1.2rem;
            font-weight: 500;
        }

        .button-group {
            display: flex;
            gap: 0.5rem;
        }

        .clear-btn, .debug-btn {
            background-color: #666;
            color: white;
            border: none;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.3s ease;
        }

        .clear-btn:hover, .debug-btn:hover {
            background-color: #555;
        }

        .transcription-content {
            background-color: #1a1a1a;
            border: 1px solid #444;
            border-radius: 4px;
            padding: 1rem;
            flex: 1;
            overflow-y: auto;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.95rem;
            line-height: 1.6;
            white-space: pre-wrap;
            word-wrap: break-word;
        }

        .transcription-content::-webkit-scrollbar {
            width: 8px;
        }

        .transcription-content::-webkit-scrollbar-track {
            background: #1a1a1a;
        }

        .transcription-content::-webkit-scrollbar-thumb {
            background: #666;
            border-radius: 4px;
        }

        .transcription-content::-webkit-scrollbar-thumb:hover {
            background: #888;
        }

        .error-message {
            background-color: rgba(244, 67, 54, 0.1);
            border: 1px solid #f44336;
            color: #ff8a80;
            padding: 1rem;
            border-radius: 4px;
            margin-bottom: 1rem;
            display: none;
        }

        .debug-info {
            background-color: rgba(255, 193, 7, 0.1);
            border: 1px solid #FFC107;
            color: #FFD54F;
            padding: 1rem;
            border-radius: 4px;
            margin-bottom: 1rem;
            font-size: 0.85rem;
            font-family: monospace;
            display: none;
        }

        .pending-text {
            color: #888;
            font-style: italic;
        }

        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            .api-key-input {
                width: 200px;
            }

            .status-bar {
                padding: 1rem;
            }

            .transcription-area {
                height: calc(100vh - 350px);
                min-height: 300px;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>üéôÔ∏è Web Whisper</h1>
    </div>

    <div class="container">
        <div class="status-bar">
            <div class="status-indicator">
                <div class="status-dot" id="statusDot"></div>
                <span id="statusText">Inactive</span>
                <div class="audio-level">
                    <div class="audio-level-bar" id="audioLevelBar"></div>
                </div>
            </div>
            <div class="api-key-section">
                <input 
                    type="password" 
                    id="apiKeyInput" 
                    class="api-key-input" 
                    placeholder="Enter OpenAI API Key"
                >
                <button id="saveKeyBtn" class="save-key-btn">Save</button>
            </div>
        </div>

        <div class="error-message" id="errorMessage"></div>
        <div class="debug-info" id="debugInfo"></div>

        <div class="transcription-area">
            <div class="transcription-header">
                <h2>Transcription History</h2>
                <div class="button-group">
                    <button id="debugBtn" class="debug-btn">Debug</button>
                    <button id="clearBtn" class="clear-btn">Clear</button>
                </div>
            </div>
            <div class="transcription-content" id="transcriptionContent">
                Waiting for microphone access...
            </div>
        </div>
    </div>

    <script>
        class WhisperTranscriber {
            constructor() {
                this.apiKey = localStorage.getItem('openai_api_key') || '';
                this.mediaRecorder = null;
                this.audioChunks = [];
                this.isRecording = false;
                this.transcriptionHistory = '';
                this.audioContext = null;
                this.analyser = null;
                this.microphone = null;
                this.scriptProcessor = null;
                this.hasSpokenInChunk = false;
                this.chunkStartTime = null;
                this.debugMode = false;
                this.audioStream = null;
                
                // Continuous recording variables
                this.continuousAudioData = [];
                this.lastTranscriptionEndTime = 0;
                this.overlapDuration = 2000; // 2 seconds overlap
                this.chunkDuration = 10000; // 10 seconds chunks
                this.silenceTimeout = null;
                this.isSpeaking = false;
                this.pendingTranscription = '';
                
                this.initializeElements();
                this.setupEventListeners();
                this.checkApiKey();
            }

            initializeElements() {
                this.statusDot = document.getElementById('statusDot');
                this.statusText = document.getElementById('statusText');
                this.apiKeyInput = document.getElementById('apiKeyInput');
                this.saveKeyBtn = document.getElementById('saveKeyBtn');
                this.clearBtn = document.getElementById('clearBtn');
                this.debugBtn = document.getElementById('debugBtn');
                this.transcriptionContent = document.getElementById('transcriptionContent');
                this.errorMessage = document.getElementById('errorMessage');
                this.audioLevelBar = document.getElementById('audioLevelBar');
                this.debugInfo = document.getElementById('debugInfo');

                if (this.apiKey) {
                    this.apiKeyInput.value = this.apiKey;
                }
            }

            setupEventListeners() {
                // API Key management
                this.saveKeyBtn.addEventListener('click', () => this.saveApiKey());
                this.apiKeyInput.addEventListener('keypress', (e) => {
                    if (e.key === 'Enter') this.saveApiKey();
                });

                // Clear button
                this.clearBtn.addEventListener('click', () => this.clearTranscription());
                
                // Debug button
                this.debugBtn.addEventListener('click', () => this.toggleDebug());

                // Tab visibility
                document.addEventListener('visibilitychange', () => {
                    if (document.hidden) {
                        this.stopRecording();
                    } else if (this.apiKey) {
                        this.startRecording();
                    }
                });

                // Window focus
                window.addEventListener('focus', () => {
                    if (this.apiKey && !this.isRecording) {
                        this.startRecording();
                    }
                });

                window.addEventListener('blur', () => {
                    this.stopRecording();
                });
            }

            toggleDebug() {
                this.debugMode = !this.debugMode;
                this.debugInfo.style.display = this.debugMode ? 'block' : 'none';
                this.debugBtn.textContent = this.debugMode ? 'Hide Debug' : 'Debug';
            }

            updateDebugInfo(info) {
                if (this.debugMode) {
                    this.debugInfo.textContent = `Debug Info:\n${info}`;
                }
            }

            checkApiKey() {
                if (this.apiKey) {
                    this.startRecording();
                } else {
                    this.updateStatus('No API Key', 'error');
                    this.transcriptionContent.textContent = 'Please enter your OpenAI API key to start transcribing.';
                }
            }

            saveApiKey() {
                const key = this.apiKeyInput.value.trim();
                if (!key) {
                    this.showError('Please enter a valid API key');
                    return;
                }

                this.apiKey = key;
                localStorage.setItem('openai_api_key', key);
                this.hideError();
                this.transcriptionContent.textContent = 'API key saved. Starting transcription...';
                this.startRecording();
            }

            async startRecording() {
                if (this.isRecording || !this.apiKey) return;

                try {
                    // Request microphone with specific constraints
                    const stream = await navigator.mediaDevices.getUserMedia({ 
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true,
                            sampleRate: 48000
                        } 
                    });
                    
                    this.audioStream = stream;
                    
                    // Setup audio context for level monitoring
                    this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    this.analyser = this.audioContext.createAnalyser();
                    this.microphone = this.audioContext.createMediaStreamSource(stream);
                    
                    // Create script processor for real-time audio analysis
                    this.scriptProcessor = this.audioContext.createScriptProcessor(2048, 1, 1);
                    
                    // Connect the nodes
                    this.microphone.connect(this.analyser);
                    this.analyser.connect(this.scriptProcessor);
                    this.scriptProcessor.connect(this.audioContext.destination);
                    
                    // Setup analyser
                    this.analyser.fftSize = 1024;
                    this.analyser.smoothingTimeConstant = 0.3;
                    
                    // Monitor audio levels
                    this.monitorAudioLevel();

                    // Get supported mime type
                    const mimeType = this.getSupportedMimeType();
                    this.updateDebugInfo(`Mime type: ${mimeType}\nAudio Context State: ${this.audioContext.state}`);
                    
                    // Create continuous recorder
                    this.mediaRecorder = new MediaRecorder(stream, {
                        mimeType: mimeType
                    });

                    this.mediaRecorder.ondataavailable = (event) => {
                        if (event.data.size > 0) {
                            this.continuousAudioData.push({
                                data: event.data,
                                timestamp: Date.now()
                            });
                        }
                    };

                    this.isRecording = true;
                    this.mediaRecorder.start(100); // Collect data every 100ms
                    this.updateStatus('Recording', 'active');
                    
                    // Start processing loop
                    this.startProcessingLoop(mimeType);

                    if (this.transcriptionHistory === '') {
                        this.transcriptionContent.textContent = 'Listening... (Speak naturally - I\'ll handle long sentences)';
                    }

                } catch (error) {
                    console.error('Error accessing microphone:', error);
                    this.showError(`Failed to access microphone: ${error.message}`);
                    this.updateStatus('Error', 'error');
                }
            }

            startProcessingLoop(mimeType) {
                this.processingInterval = setInterval(async () => {
                    if (!this.isRecording) return;

                    // Check if we have enough audio data
                    if (this.continuousAudioData.length === 0) return;

                    const now = Date.now();
                    const oldestTimestamp = this.continuousAudioData[0].timestamp;
                    
                    // Process if we have enough data OR if there's been silence after speech
                    const hasEnoughData = (now - oldestTimestamp) >= this.chunkDuration;
                    const hasSilenceAfterSpeech = !this.isSpeaking && this.hasSpokenInChunk && 
                                                   (now - this.lastSpeechTime) > 1500;

                    if (hasEnoughData || hasSilenceAfterSpeech) {
                        await this.processAudioChunk(mimeType);
                    }
                }, 1000); // Check every second
            }

            async processAudioChunk(mimeType) {
                if (this.continuousAudioData.length === 0) return;

                // Calculate overlap point
                const now = Date.now();
                const overlapStartTime = this.lastTranscriptionEndTime - this.overlapDuration;
                
                // Get chunks for transcription
                const chunksToProcess = [];
                let processEndTime = now;
                
                for (const chunk of this.continuousAudioData) {
                    if (chunk.timestamp >= overlapStartTime) {
                        chunksToProcess.push(chunk.data);
                    }
                    processEndTime = chunk.timestamp;
                }

                if (chunksToProcess.length === 0) return;

                // Create blob from chunks
                const audioBlob = new Blob(chunksToProcess, { type: mimeType });
                
                this.updateDebugInfo(`Processing chunk: ${audioBlob.size} bytes\nOverlap: ${this.overlapDuration}ms\nChunks: ${chunksToProcess.length}`);

                // Transcribe with context
                const transcription = await this.transcribeAudioWithContext(audioBlob, mimeType);
                
                if (transcription) {
                    // Handle the transcription with overlap detection
                    this.handleOverlappingTranscription(transcription, overlapStartTime > 0);
                    this.lastTranscriptionEndTime = processEndTime;
                }

                // Clean up old audio data
                this.continuousAudioData = this.continuousAudioData.filter(
                    chunk => chunk.timestamp > processEndTime - this.overlapDuration
                );
                
                this.hasSpokenInChunk = false;
            }

            async transcribeAudioWithContext(audioBlob, mimeType) {
                try {
                    const audioFile = new File([audioBlob], 'audio.webm', { 
                        type: mimeType 
                    });

                    const formData = new FormData();
                    formData.append('file', audioFile);
                    formData.append('model', 'whisper-1');
                    formData.append('response_format', 'verbose_json');
                    formData.append('language', 'en');
                    
                    // Add prompt for context if we have pending transcription
                    if (this.pendingTranscription) {
                        formData.append('prompt', this.pendingTranscription);
                    }

                    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                        method: 'POST',
                        headers: {
                            'Authorization': `Bearer ${this.apiKey}`
                        },
                        body: formData
                    });

                    if (!response.ok) {
                        const error = await response.json();
                        throw new Error(error.error?.message || 'Transcription failed');
                    }

                    const result = await response.json();
                    return result;

                } catch (error) {
                    console.error('Transcription error:', error);
                    this.showError(`Transcription error: ${error.message}`);
                    return null;
                }
            }

            handleOverlappingTranscription(result, hasOverlap) {
                const transcription = result.text;
                
                if (!transcription || transcription.trim().length < 2) return;

                // Filter out noise
                if (transcription.match(/^(you|You|Yeah|yeah|Hmm|hmm|Mm|mm|Um|um|Uh|uh)\.?$/)) return;

                if (hasOverlap && this.pendingTranscription) {
                    // Find the overlap point by looking for common words
                    const overlap = this.findOverlapPoint(this.pendingTranscription, transcription);
                    
                    if (overlap !== -1) {
                        // Remove the overlapping part from the new transcription
                        const newPart = transcription.substring(overlap).trim();
                        if (newPart) {
                            this.addTranscription(newPart, true);
                            this.pendingTranscription = this.pendingTranscription + ' ' + newPart;
                        }
                    } else {
                        // No clear overlap found, add the whole transcription
                        this.addTranscription(transcription, false);
                        this.pendingTranscription = transcription;
                    }
                } else {
                    // First transcription or after silence
                    this.addTranscription(transcription, false);
                    this.pendingTranscription = transcription;
                }

                // Keep only last 100 words for context
                const words = this.pendingTranscription.split(' ');
                if (words.length > 100) {
                    this.pendingTranscription = words.slice(-100).join(' ');
                }
            }

            findOverlapPoint(previous, current) {
                // Simple word-based overlap detection
                const prevWords = previous.toLowerCase().split(' ');
                const currWords = current.toLowerCase().split(' ');
                
                // Look for the last few words of previous in current
                for (let i = Math.min(10, prevWords.length); i >= 3; i--) {
                    const lastWords = prevWords.slice(-i).join(' ');
                    const index = current.toLowerCase().indexOf(lastWords);
                    if (index !== -1) {
                        return index + lastWords.length;
                    }
                }
                
                return -1;
            }

            getSupportedMimeType() {
                const types = [
                    'audio/webm;codecs=opus',
                    'audio/webm',
                    'audio/ogg;codecs=opus',
                    'audio/mp4',
                ];
                
                for (const type of types) {
                    if (MediaRecorder.isTypeSupported(type)) {
                        console.log('Using mime type:', type);
                        return type;
                    }
                }
                
                return 'audio/webm'; // fallback
            }

            monitorAudioLevel() {
                if (!this.analyser || !this.isRecording) {
                    this.audioLevelBar.style.width = '0%';
                    return;
                }

                const bufferLength = this.analyser.fftSize;
                const dataArray = new Uint8Array(bufferLength);
                this.analyser.getByteTimeDomainData(dataArray);
                
                // Calculate RMS (Root Mean Square) for better volume representation
                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    const normalized = (dataArray[i] - 128) / 128;
                    sum += normalized * normalized;
                }
                const rms = Math.sqrt(sum / bufferLength);
                
                // Convert to percentage (0-100)
                const percentage = Math.min(100, rms * 500);
                this.audioLevelBar.style.width = percentage + '%';
                
                // Detect if there's significant audio (speaking)
                const wasSpeaking = this.isSpeaking;
                this.isSpeaking = percentage > 5;
                
                if (this.isSpeaking) {
                    this.hasSpokenInChunk = true;
                    this.lastSpeechTime = Date.now();
                }
                
                // Update debug info
                if (this.debugMode) {
                    const bufferSize = this.continuousAudioData.length;
                    const oldestTime = bufferSize > 0 ? 
                        ((Date.now() - this.continuousAudioData[0].timestamp) / 1000).toFixed(1) : 0;
                    this.updateDebugInfo(
                        `Audio Level: ${percentage.toFixed(1)}%\n` +
                        `Speaking: ${this.isSpeaking}\n` +
                        `Buffer: ${bufferSize} chunks (${oldestTime}s)\n` +
                        `Pending context: ${this.pendingTranscription ? 
                            this.pendingTranscription.split(' ').length + ' words' : 'none'}`
                    );
                }
                
                // Continue monitoring
                requestAnimationFrame(() => this.monitorAudioLevel());
            }

            stopRecording() {
                if (!this.isRecording) return;

                this.isRecording = false;
                
                if (this.processingInterval) {
                    clearInterval(this.processingInterval);
                    this.processingInterval = null;
                }

                if (this.mediaRecorder && this.mediaRecorder.state !== 'inactive') {
                    this.mediaRecorder.stop();
                }

                if (this.audioStream) {
                    this.audioStream.getTracks().forEach(track => track.stop());
                    this.audioStream = null;
                }

                if (this.scriptProcessor) {
                    this.scriptProcessor.disconnect();
                    this.scriptProcessor = null;
                }

                if (this.microphone) {
                    this.microphone.disconnect();
                    this.microphone = null;
                }

                if (this.analyser) {
                    this.analyser.disconnect();
                    this.analyser = null;
                }

                if (this.audioContext) {
                    this.audioContext.close();
                    this.audioContext = null;
                }

                // Clear audio data
                this.continuousAudioData = [];
                this.pendingTranscription = '';
                this.lastTranscriptionEndTime = 0;

                this.audioLevelBar.style.width = '0%';
                this.updateStatus('Stopped', 'inactive');
                this.updateDebugInfo('Recording stopped');
            }

            addTranscription(text, isContinuation) {
                const timestamp = new Date().toLocaleTimeString();
                let formattedText;
                
                if (isContinuation && this.transcriptionHistory.length > 0) {
                    // Add as continuation without timestamp
                    formattedText = ` ${text.trim()}`;
                } else {
                    // Add with timestamp
                    formattedText = `\n\n[${timestamp}] ${text.trim()}`;
                }
                
                this.transcriptionHistory += formattedText;
                this.transcriptionContent.textContent = this.transcriptionHistory.trim();
                
                // Auto-scroll to bottom
                this.transcriptionContent.scrollTop = this.transcriptionContent.scrollHeight;
            }

            clearTranscription() {
                this.transcriptionHistory = '';
                this.pendingTranscription = '';
                this.transcriptionContent.textContent = this.isRecording ? 
                    'Listening... (Speak naturally - I\'ll handle long sentences)' : 
                    'Cleared. Start recording to see new transcriptions.';
            }

            updateStatus(text, state) {
                this.statusText.textContent = text;
                this.statusDot.className = `status-dot ${state}`;
            }

            showError(message) {
                this.errorMessage.textContent = message;
                this.errorMessage.style.display = 'block';
            }

            hideError() {
                this.errorMessage.style.display = 'none';
            }
        }

        // Initialize the app when the page loads
        const app = new WhisperTranscriber();
    </script>
</body>
</html>
